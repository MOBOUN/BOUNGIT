---
title: "MACHINE LEARNING CASE-MOVIE RATING PREDICTION"
output:
  word_document: default
  pdf_document: default
date: "2023-12-02"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1 INTRODUCTION: OBJECTIVE, CONTEXT AND SCOPE

The aim of current project is to create a movie recommendation system using the 10M version of the MovieLens; we'll be using some tools we have learned throughout the courses, especially R coding, Data exploration and visualization and machine learning algorithms.

We will use the 10M version of the MovieLens dataset to make the computation feasible and easy to be evaluated; the entire latest MovieLens dataset is larger and kept up to date on grouplens.org website. the process followed to achieve this project can be summarized in the following steps:

Extraction Data Pre-processing Data Exploration Solution Modeling and coding Results Conclusion

In addition to courses I learned on Edx platform, other internet sources were very helpful for me understanding each of the steps and I pointed the most used to better understand machine learning process and algorithms, and to implement blocs of code in R as well. The most used resources are pointed in the references section of pdf report My thoughts beyond the academic aspect of this project are opened to discussion on section 5 of pdf report. the scope been defined clearly withing the "capstone" instructions: finding the best model that minimizes RMSE, regularization included been one of the alternatives. The Output to predict, rating, is categorical with 10 possible levels, thus we will test linear regression based algorithms and find out the most accurate.

# 2 Extraction: Code provided by the EDX staff to download an create MovieLens dataset.

```{r Extraction, message = FALSE, warning = FALSE}

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(anytime)) install.packages("anytime", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(dplyr)
library(anytime)
library(lubridate)
library(ggplot2)

###DOWNLOADING "movielens/ml-10m.zip" and unzipping to "ratings" and "movies" files ###

options(timeout = 120)

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
        col.names = c("userId", "movieId", "rating", "timestamp"))

ratings <- ratings %>%
  mutate(userId = as.integer(userId),
         movieId = as.character(movieId),
         rating = as.numeric(rating),
         timestamp = as.integer(timestamp))


movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::",3)
           colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.character(movieId),
                                             title = as.character(title),
                                             genres = as.character(genres))

##################### Check download "ratings" and "movies"
glimpse(ratings)
glimpse(movies)
#################################### Join "ratings" and "movies" to "movielens" #####
movielens <- left_join(ratings, movies, by = "movieId")
glimpse(movielens)
####looking for incomplete entries (missing data)
sum(is.na(movies))
sum(is.na(ratings))
sum(is.na(movielens))
```

```{r Partition1, message = FALSE, warning = FALSE}
### "final_holdout_test" set, for validation, will be 10% of Movielens data, "edx" will be used for the training and test

set.seed(1, sample.kind = "Rounding") #  actual version R 4.2.3

test_index <- caret::createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]
  
  # Make sure userId and movieId in final_holdout_test set are in "edx" set
  
final_holdout_test <- temp %>% 
    semi_join(edx, by = "movieId") %>%
    semi_join(edx, by = "userId")
  
  # Add rows removed from final_holdout_test back to edx set

removed <- anti_join(temp, final_holdout_test)
edx <- rbind(edx, removed)

###looking for incomplete entries (missing data)
sum(is.na(final_holdout_test))  
sum(is.na(edx))
```

#### Training and validation sets Overview

```{r TRAIN AND VALIDATION SETS OVERVIEW, message = FALSE, warning = FALSE}
############# key figures ##########################################
glimpse(edx)
glimpse(final_holdout_test)
summary(edx)
summary(final_holdout_test)

#how many different users
length(unique(edx$userId))
#how many different movies
length(unique(edx$movieId))
```

# 3 DATA PREPROCESSING

no missing data found in any of the input and output files of the training and validation sets\
sum(is.na(movies)) = 0 sum(is.na(ratings))= 0 sum(is.na(movielens))= 0 sum(is.na(final_holdout_test))= 0 sum(is.na(edx))=0

*Original "movielens" contains*
Rows: 10,000,054 Columns: 6 
userId <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... 
movieId <int> 122, 185, 231, 292, 316, 329, 355, 356, 362, 3...
rating <dbl> 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5... 
timestamp <int> 838985046, 838983525, 838983392, 838983421, 83...
title <chr> "Boomerang (1992)", "Net, The (1995)", "Dumb &...
genres <chr>"Comedy\|Romance", "Action\|Crime\|Thriller", "Co...

*The edx set contains*

9000055 ratings provides by 69878 unique users for 10677 unique evaluated movies

Rows: 9,000,055 Columns: 6 observations 
\$ userId <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 
\$ movieId <int> 122, 185, 292, 316, 329, 355, 356, 362, 364, 370, 377, 420, 466, 520,...
\$ rating <dbl> 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 3, 5, 2, 3,... 
\$ timestamp <int> 838985046, 838983525, 838983421, 838983392, 838983392, 838984474, 838983653,...
\$ title <chr> "Boomerang (1992)", "Net, The (1995)", "Outbreak (1995)", "Stargate (1994)", "St... \$ genres <chr>"Comedy\|Romance", "Action\|Crime\|Thriller", "Action\|Drama\|Sci-Fi\|Thriller",...

There are 69878 unique users given ratings to 10677 different films. the unique genres were counted `Drama` and `Comedy|Drama` are counted as 2 different genres.same remark for `Comedy`and`Comedy|Drama` where we count `Comedy` 2 times etc 

"rating", "genres" and title are in the character format The `userId` and `movieId` variables are `Integer` format in the original data set.These are actually labels and are converted to `factor` type in the following section.

Rating given by a user to a movie values From 0 to 5 stars stepping of 0.5.type is `numeric`
Having in mind the fact that a time effect could be ovserved on rating, or should we look at this potential effect, we need to change "timestamp" format to be useful, I also loaded lubridate and anytime libraries for this aim.The `timestamp` variable is converted to `POSIXct` type, to be handle correctly as a `date` vector. The year is extracted to the `year` column..

*The final_holdout_test set contains:* 
Rows: 999,999 records Columns: 6 observations 

```{r formats change, message = FALSE, warning = FALSE}
edx$userId <- as.factor(edx$userId) # change `userId` to `factor`.
edx$movieId <- as.factor(edx$movieId) # change `movieId` to `factor`.
edx$genres <- as.factor(edx$genres) # change `genres` to `factor`.
edx$timestamp <- as.POSIXct(edx$timestamp, origin = "1970-01-01") # change `timestamp to `POSIXct`
edx <- edx %>% mutate(year_rate = year(timestamp)) #  add year OF rating from timestamp.
glimpse(edx)
```

# 4 DATA EXPLORATION

```{r rating_summary, message = FALSE, warning = FALSE}
#the most rated title 
Firsttitle <- edx%>% select(title,userId)%>%
  group_by(title)%>%summarise(nbuser=n())%>%
  arrange(desc(nbuser))
head(Firsttitle)

# Rating top down
distrib <- edx %>% select(rating,userId)%>%
  group_by(rating)%>%summarise(nbuser=n())%>%
  select(rating,nbuser)%>%
  arrange(desc(nbuser))
head(distrib,10)

```

```{r Ratingdistribution, message = FALSE, warning = FALSE}

# Rating distribution using geom_bar

rat <- as.data.frame <- edx %>% select(rating,userId)%>%
  group_by(rating)%>%summarise(nbusers=n())%>%
  select(rating,nbusers) %>% mutate(rating=as.character(rating))
ggplot(rat,aes(x=factor(rating),y=nbusers))+
geom_bar(stat='Identity',fill=rat$rating)

```

The most given rates are respectively: 4/3/5/3.5/2; More generally We notice left-skewed shape of rating distribution. there are more *likes* ratings than *bad ratings* ratings. Neuro-scientists or even psychology could explain better this behavior, furthermore find normal this shape, as people tend to recommend their own preferences to their friends and families. We can also assume that ranking a movie, after watching it entirely, means we took time for this, we were interested and even liked before the end. In the other hand, disliking a film could happen at some point while watching. The normal behavior when we don't like,is the to quit, without having in complete idea to evaluate, nor the time and will to spend on commenting and evaluations. People who have rated small values less than 1.5 could be interested on some aspects of a movie at the begging(director or actors reputations, genre fo the film or social media influenced, professionals of the movie industry, journalists, and everyone disappointed regarding his initial expectations. We can make more assumptions on the impact of the scale itself, as soon as there's clearly more rates with entire numbers than half rates, could we imagine the distribution withe INTEGERs from (1:5) or (1:10), behavioral or neurological science should have more insights

#Analazing the "year of release" effect on movie rating
There's a record of 15 years from 1995 to 2009, These 2 limints of the range seem to be 
#outliners
As far as few records were done, the ratings didn't include the hole year for 1995 AND 2009

```{r year_rate_plot, message = FALSE}
table(unique(edx$year_rate))
edx %>% ggplot(aes(year_rate)) +
  geom_histogram(binwidth=1,color="darkblue",fill="#56B4E9") +
  labs(title = "Distribution of the year_rate",
       subtitle = "Ratings per year",
       x = "Year",
       y = "Frequency")
```

The shape observed from rating frequency to release year shows spikes on 1996 followed by decreasing numbers for 1997 and 1998, after that, at the 3rd year (after the spike), ratings start the rise. Same remark 2000 and 2005, It seems there's a five year cycle from spike to spike,2 years after the left spike are decreasing, the growth starts from the 4th year. However, more data is needed to confirm this hypothesis, which is out of scope of this work. the rating seem to fluctuate less between 2 spikes

```{r  Rating distribution, warning=FALSE, error=FALSE}
# Rating distribution top down
distyear <- edx %>% select(userId,year_rate)%>%
  group_by(year_rate)%>%summarise(yearcount=n())%>%
  arrange(desc(yearcount))
head(distyear,15)
sum(distyear$yearcount)
```

## rating breakdown per genre

797 different genres were listed, with overlapping counts as one movie could have been assigned more than one specific genre (drama, comedy ...), in fact most of the movies are hardly classified within a single genre (in one single word). the figures show different conclusions depending whether we arrange the rating vs genre by the average (men(rating) or the total number of rating: "Animation\|IMAX\|Sci-Fi" for example is rated only 7 times with relatively high average "Action\|Drama\|Thriller\|War" for example is rated only 480 times with relatively high average in the other hand the most rated genres are "Drama" and "Comedy" regardless of combined genres that may contain these 2 single classified genres. In the other hand, the 733 296 ratings for Drama genre don't include all the occurrences of drama type when it's mergend in combined categories as "Comedy\|Drama" and Comedy\|Drama\|Romance ...

```{r genres_distribution, warning=FALSE, error=FALSE }
distgen <- edx %>%
  select(genres, rating) %>%
  group_by(genres) %>%
  summarize(avg = mean(rating), nbpg = n())

distgen%>% arrange(desc(avg)) %>% head(20)
distgen%>% arrange(desc(nbpg)) %>% head(20)

```

## count genres appearances with all occurrences of single word.

We use grepl() function to look for every occurrence of the word "Drama" through genres column and do the same for "Comedy","Thriller" and "Romance" categories, then we put the data in a table a make a pie chart. notice the total ratings number of these 4 items is 11 489 056 is larger than the total count of ratings, because in this case, the same rating is counted more than one time, as much as the number of single categories of the rated movie.

```{r count ratings per single genres, warning = FALSE,error=FALSE}

drama<-sum(grepl("Drama",edx$genres))
comedy<-sum(grepl("Comedy",edx$genres))
thriller<-sum(grepl("Thriller",edx$genres))
romance<-sum(grepl("Romance",edx$genres))

#### TABLE GENRE VS Nb RATINGS
GenTable1 <-data.frame(
  RATINGS=c(drama,comedy,thriller,romance),
  GENRE=c("Drama","Comedy","Thriller","Romance"))
glimpse(GenTable1)

#### chart GENRE VS Nb RATINGS
ggplot(GenTable1,aes(x ="",y=RATINGS,fill=GENRE,labels=GENRE))+
geom_bar(width = 1, stat = "identity")+
coord_polar("y", start=0)+
scale_fill_brewer(palette="Blues")+
theme_minimal()+
  theme(
    axis.title.y = element_blank(),
    panel.border = element_blank(),
    panel.grid=element_blank(),
    axis.ticks = element_blank(),
    plot.title=element_text(size=14, face="bold"))

```
# 5. MODELING

the aim is to find out a learning model with the minimum error for prediction; as a success criteria we'll try to get RMSE less than 0.86490 that give right to 25 POINTS evaluation and seem to be a good achievement from previous submitted projects. both training and validation sets are created above with the provided code from EDX staff. Rows: 999,999 records and Columns: 6 observations "final_holdout_test" will be kept unchanged for final estimation of RMSE only (Validation), with the last model choosen with the best RMSE on "test" set, The edx set contains initially 9000055 ratings provides by 69878 unique users for 10677 unique evaluated movies, will be split in 2 subsets: "training" and "test"

```{r Partition training test, warning=FALSE, error=FALSE }
edx <- edx %>% select(userId, movieId, rating)
test_index <- createDataPartition(edx$rating, times = 1, p = .2, list = F)# Create the index
training <- edx[-test_index, ] # Create Train set
test <- edx[test_index, ] # Create Test set
test <- test %>% # The same movieId and usersId appears in both set. (Not the same cases)
    semi_join(training, by = "movieId") %>%
    semi_join(training, by = "userId")

rmse_table <-0 
head(rmse_table)
glimpse(edx)
glimpse(test)
dim(training)
dim(test)
dim(edx)
```

subset "test" has now 1 799 967 rows, "training" has 7 200 043 obs and edx 9 000 055 rows
I created RMSEs dataframe used to collect gradually all the values for tested models with meodel name and RMSE value obtained

## Baseline model

The simplest model is to use the most common rating from the *training* set to be predicted. This is the **baseline MODEL** model.

```{r  baseline, message = FALSE, warning = FALSE}
mu <- mean(training$rating) # Mean accross all movies.
model0_RMSE <- RMSE(test$rating, mu) # RMSE in test set.
model0_RMSE
```

The RMSE baseline model 1.060289 is greater than 1 and thus show the baseline model is weak for making prediction, more than 1 star error; we looked for better models in the following section. RMSEs for different models are collected in rmse_table we created with the code bellow:

```{r   RMSE BASELINE, message = FALSE, warning = FALSE}
rmse_table <- data_frame(Method = "Baseline MODEL", RMSE = model0_RMSE)
rmse_table %>% knitr::kable(caption = "RMSEs")
```

## Movie effect Model\*

```{r Movie effect Model, warning=FALSE, error=FALSE}
Mu <- mean(training$rating)

movieAVG <- training %>%mutate(movieId=as.factor(movieId))%>%
  group_by(movieId) %>%
  summarize(Mi = mean(rating - Mu))

PREDICTIONS <- test %>%
  left_join(movieAVG, by = "movieId") %>%
  mutate(pred = Mu + Mi) %>% .$pred

model1_RMSE <- RMSE(PREDICTIONS,test$rating)
model1_RMSE  

```

```{r RMSE 2, message = FALSE, warning = FALSE}
rmse_table <- rbind(rmse_table, data_frame(Method = "Movie Effect", RMSE = model1_RMSE))
rmse_table %>% knitr::kable(caption = "RMSEs")
```

*User effect Model*

```{r User effect Model,error=FALSE, message=FALSE}
Mu <- mean(training$rating)

userEFFAVG <- training %>%mutate(userId=as.factor(userId))%>%
  group_by(userId) %>%
  summarize(Mi = mean(rating - Mu))

PREDICTIONS <- test %>%
  left_join(userEFFAVG, by = "userId") %>%
  mutate(pred = Mu + Mi) %>% .$pred

model2_RMSE <- RMSE(PREDICTIONS,test$rating)
model2_RMSE
```

```{r RMSE 3, message = FALSE, warning = FALSE}
rmse_table <- rbind(rmse_table, data_frame(Method = "User Effect", RMSE = model2_RMSE))
rmse_table %>% knitr::kable(caption = "RMSEs")
```

## User and Movie effect Model

User effect model and movie effect model both have still a big RMSE and weeak prediction power, whereas the error is bigger when using user effect alone

The next step is going to try to get a new model with a better RMSE. using both user and movie effect. We take the user effect *Ui* and the movie effect *Mi* as predictors. Therefore, we are generating the next model to predict `rating` *y_hat_i*: *y_hat_i=Ui+Mi+epsilon*

```{r User and Movie effect Model, message = FALSE, warning = FALSE}
Mu <- mean(training$rating)

movieAVG <- training %>%mutate(movieId=as.factor(movieId))%>%
  group_by(movieId) %>%
  summarize(Mi = mean(rating - Mu))

userAVG <- training %>% mutate(movieId=as.factor(movieId))%>%
  left_join(movieAVG, by = "movieId") %>%
  group_by(userId) %>%
  summarize(Ui = mean(rating - Mu - Mi))

PREDICTIONS <- test %>%
  left_join(movieAVG, by = "movieId") %>%
  left_join(userAVG, by = "userId") %>%
  mutate(pred = Mu + Mi + Ui) %>% .$pred

modelMU_RMSE <- RMSE(PREDICTIONS,test$rating)
modelMU_RMSE  
```

```{r RMSE 4, message = FALSE, warning = FALSE}
rmse_table <- rbind(rmse_table,data_frame(Method = "User & Movie Effect", RMSE = modelMU_RMSE))
rmse_table %>% knitr::kable(caption = "RMSEs")
```

We've got obtained a better RMSE when using this model than when using user effect or movie effect alone. this is expect because the effect of both is obvious on the prediction for all reasons discussed above. *but* we are still away from our target, let's see if \*\*regularisation\* will give us better results.

## ###Regularisation WITH MOVIE & USER EFFECT

The regularisation will evaluate different values for *lambda*, and show us the value corresponding RMSE minimal. The best Lambda found from early runs is about 4.5, thus we set 3:6 range for the bellow sapply function to go faster. from early runs is about 4.5, thus we set 3:6 range for the bellow sapply range to go faster.

```{r Regularization, message = FALSE, warning = FALSE}

lambda_values <- seq(3,6,.5)

RMSE_function_reg <- sapply(lambda_values, function(l){
  
  mu <- mean(training$rating)
  
  Mi <- training %>%
    group_by(movieId) %>%
    summarize(Mi = sum(rating - mu)/(n()+l))
  
  Ui <- training %>%
    left_join(Mi, by="movieId") %>%
    group_by(userId) %>%
    summarize(Ui = sum(rating - Mi - mu)/(n()+l))
  
  predicted_ratings <- test %>%
    left_join(Mi, by = "movieId") %>% 
    left_join(Ui, by = "userId") %>%
    mutate(pred = mu + Mi + Ui) %>% .$pred
  
  return(RMSE(predicted_ratings, test$rating))
})

qplot(lambda_values, RMSE_function_reg,
      main = "Regularisation",
      xlab = "Lambda", ylab ="RMSE" ) # lambda vs RMSE
lambda_opt <- lambda_values[which.min(RMSE_function_reg)]# Lambda which minimizes RMSE
lambda_opt 
```

```{r RMSE 5, message = FALSE, warning = FALSE}
rmse_table <- rbind(rmse_table, 
                    data_frame(Method = "User & Movie Effect Regularisation, test set ",
                    RMSE = min(RMSE_function_reg)))
rmse_table %>% knitr::kable(caption = "RMSEs")
```

## PREDICTIONS

The regularisation give as a RMSE than the first "User & Movie Effect" model. This is expected, this models takes in account the combined effects most important, gave an RMSE better than all previous models 0.8659116

NOW LET PREDICT USING REGULARISATION OUTCOME, we will estimate the LMSE when predictions are confronted to final_holdout_test results We need to change variables formats on *final_holdout_test* data set like we did for *edx* data set :

```{r final_holdout_test, message = FALSE, warning = FALSE}
final_holdout_test <- final_holdout_test %>% 
  mutate( userId = as.factor(userId),
      movieId=as.factor(movieId))
glimpse(final_holdout_test)
```

*we run the model above to estimate RMSE on final_holdout_test*

```{r PREDICTIONS, message = FALSE, warning = FALSE, echo = FALSE}
mu <- mean(training$rating)

movieAVG <- training %>%
  group_by(movieId) %>%
  summarize(Mi = mean(rating - mu))

userAVG <- training %>%
  left_join(movieAVG, by = "movieId") %>%
  group_by(userId) %>%
  summarize(Ui = mean(rating - mu- Mi))

PREDICTIONS <- final_holdout_test %>%
  left_join(movieAVG, by = "movieId") %>%
  left_join(userAVG, by = "userId") %>%
  mutate(pred = mu + Ui + Mi) %>% .$pred

val_RMSE <- RMSE(PREDICTIONS, final_holdout_test$rating, na.rm = TRUE)
val_RMSE
```

```{r RMSE 6, message = FALSE, warning = FALSE}
rmse_table_val <- data_frame(Method = "User & Movie Effect on final houldout test",
RMSE = val_RMSE)
rmse_table <- rbind(rmse_table,
                    data_frame(Method = "User & Movie Effect on final_houldout_test", RMSE = val_RMSE))

rmse_table %>% knitr::kable(caption = "RMSEs")

```

the RMSE 0.8665063 is worse than the previous obtained on test set , note training is 80% of edx and test 20%, means test counts 1.8M rows while final_holdout_test counts 1M, another issue with under fitting could explain this result as regard to Lambda stepping and range settings,we could change this to have the same size on test and validation subsets; plus probably a better optimizer Lambda.We can also build on current model but consider the hole edx subset for training.In fact, above, we understood the Movie and user effect, with regularization on "training" set was the best model to predict values on "test" set. let's step back to this once, but now our training subset will be the hole "edx" and we make predictions and estimate with the best fitting model observed on edx set.

## *PREDICTIONS on final_holdout_test with parameters from edx set, moedl:regularized user & movie effect*

```{r FINAL PREDICTIONS, message = FALSE, warning = FALSE}
lambda_values <- seq(3,6,.5)

RMSE_function_reg <- sapply(lambda_values, function(l){
  
  mu <- mean(edx$rating)
  
  Mi <- edx %>%
    group_by(movieId) %>%
    summarize(Mi = sum(rating - mu)/(n()+l))
  
  Ui <- edx %>%
    left_join(Mi, by="movieId") %>%
    group_by(userId) %>%
    summarize(Ui = sum(rating - Mi - mu)/(n()+l))
  
  predicted_ratings <- final_holdout_test %>%
    left_join(Mi, by = "movieId") %>% 
    left_join(Ui, by = "userId") %>%
    mutate(pred = mu + Mi + Ui) %>% .$pred
  
  return(RMSE(predicted_ratings, final_holdout_test$rating))
})

qplot(lambda_values, RMSE_function_reg,
      main = "Regularisation",
      xlab = "Lambda", ylab ="RMSE" ) # lambda vs RMSE

lambda_opt <- lambda_values[which.min(RMSE_function_reg)]
lambda_opt # Lambda which minimizes RMSE
```

```{r RMSE 7, message = FALSE, warning = FALSE}
rmse_table <- rbind(rmse_table,
                    data_frame(Method = "User & Movie Effect Regularisation with edx set",
                               RMSE = min(RMSE_function_reg)))
rmse_table %>% knitr::kable(caption = "RMSEs")
```


##6. RESULTS

This is the best estimate we could find, and is on the target 0.8648177 < < 0.86490
We can observe that the better RMSE is obtained from the *User & Movie Effect* model. However, this RMSE *only* *obtained on the *training* set. When we move to the *final_holdout_test* data set, we obtain the worse RMSE (ignoring the baseline).the final test estimated error have probably suffered from under fitting while doing predictions with the regularization operation on training set (subset of edx) was not seen when we shifted to edx instead.

This confirms that parameters we choose from "edx"  are more likely to give better predictions  model, and that both user and effect, after regularization, led to the lowest RMSE,which is,better yet on target.
The processing time didn't cause any issue, even with a standard student owned computer, the regularization algorithm took few minutes, the downloading/unzipping of movie Lens file took 5 to 10 minutes with a standard home WiFi connection

##7. CONCLUSION
Both `userId` and `movieId` have  predictive power to predict how a user will rate a movie. even though movie effect seem bigger than user effect,  we then needed to take both of them in account, and saw that regularization gave better precision.
this work did permit me to test a new approach on training / validation I didn't find through literature I had time to view, this gave me good estimate: I split the edx set 
to training and test subset, I used them to choose a model, I then stepped back and applied the same model on the hole edx set, and estimated the LMSE on final_holdout_test.
This means we can merge training and validations sets on a new training set (actually old set before split), without changing the test set initially appointed to this matter.

##8 TOUGHTS
Watching time effect is theoretically significant to test for eventual impact on rating a movie, week_ends and holidays could be favorable for greater rates (at least on romance and family genres). Plus, the real world, a user  could have seen a movie more than one time and gave different rates. 
Genre effect is also not integrated in the model because lack of time, knowing that genre effect is not entirely hidden by movie effect,I guess user/genre effect would raise probably more than user/movie effect.
We can question the impact of the rating scale itself, more smooth predictions could happen over a scale with hole integers (stars).
Other aspects of recommendation system are not possible to explore with actual database, the age and gender of users, social conditions and beliefs are criteria for watching a movie, then evaluations. Film budget, duration, marketing effort deployed, director and actors reputation and history, collaborative influence with social networks are just as important regarding the evaluation of a movie. We all like to recommend a movie to relates and friends, and tend to better rate the movies we have seen. In the other hand our networks influences all our preferences.
Moreover, the audience/number of watchers versus number of evaluations is a good criteria, we don't have the watchers here, and we know that not all watchers rate a movie they watched.
Professionals and neurologists know certainly about other technical and behavioral features to include to the model and interpret its findings: technology evolution in sound and image special effects for specific genres as SCI-FI. This means more collaboration and more efficient machine leaning models are actually used for rating prediction.

#9 REFERENCES
this work was a good opportunity for me to review different modules early attended, and to revise related chapters on text book:
*https://rafalab.dfci.harvard.edu/dsbook/*

Some items came to my understanding better with videos on YouTube channels with links bellow, I thanks all those who provide pretty didactically elements for them knowledge sharing. 
The valuable ones I remember are:

*https://www.youtube.com/@designworld15*
*http://www.sthda.com/english/*
*http://www.zstatistics.com/videos/*
*https://www.youtube.com/@RProgramming101*
*https://www.geeksforgeeks.org/root-mean-square-error-in-r-programming/*
*https://towardsdatascience.com/predicting-the-survival-of-titanic-passengers-30870ccc7e8*
*https://www.youtube.com/@misraturp*